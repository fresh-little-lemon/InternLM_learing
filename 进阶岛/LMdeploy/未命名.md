# 3 LMDeploy 与 InternVL2

本次实践选用 InternVL2-26B 进行演示，其实就根本来说作为一款 VLM 和上述的 InternLM2.5 在操作上并无本质区别，仅是多出了"图片输入"这一额外步骤，但作为量化部署进阶实践，选用 InternVL2-26B 目的是带领大家体验一下 LMDeploy 的量化部署可以做到何种程度。

## 3.1 LMDeploy Lite

InternVL2-26B 需要约 70+GB 显存，但是为了让我们能够在***30%A100***上运行，需要先进行量化操作，这也是量化本身的意义所在——即降低模型部署成本。

### 3.1.1 W4A16 模型量化和部署

针对 InternVL 系列模型，让我们先进入 conda 环境，并输入以下指令，执行模型的量化工作。(本步骤耗时较长，请耐心等待)

```Plain
conda activate lmdeploy
lmdeploy lite auto_awq \
   /root/models/InternVL2-26B \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 2048 \
  --w-bits 4 \
  --w-group-size 128 \
  --batch-size 1 \
  --search-scale False \
  --work-dir /root/models/InternVL2-26B-w4a16-4bit
```

等终端输出如下时，说明正在推理中，稍待片刻。

![img](https://raw.githubusercontent.com/BigWhiteFox/pictures/main/31.png)

等待推理完成，便可以在左侧/models 内直接看到对应的模型文件。

### 3.1.2 W4A16 量化+ KV cache+KV cache 量化

输入以下指令，让我们启用量化后的模型。

```Python
lmdeploy serve api_server \
    /root/models/InternVL2-26B-w4a16-4bit \
    --model-format awq \
    --quant-policy 4 \
    --cache-max-entry-count 0.1\
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
```

启动后观测显存占用情况，此时只需要约 23.8GB 的显存，已经是一张***30%A100***即可部署的模型了。

![img](https://raw.githubusercontent.com/BigWhiteFox/pictures/main/32.png)

![img](https://raw.githubusercontent.com/BigWhiteFox/pictures/main/33.png)

根据 [InternVL2](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/) 介绍，InternVL2 26B 是由一个 6B 的 ViT、一个 100M 的 MLP 以及一个 19.86B 的 internlm 组成的。

<details>   <summary>点击显示/隐藏显存占用情况的计算细节</summary>      让我们来计算一下使用 A100 80GB 直接启动模型的显存占用情况：<br>   1、在 fp16 精度下，6BViT 模型权重占用 12GB：60×10^9 parameters×2 Bytes/parameter=12GB<br>   2、在 fp16 精度下，19.86B≈20B 的 internlm 模型权重占用 40GB：200×10^9 parameters×2 Bytes/parameter=40GB<br>   3、kv cache 占用 22.4GB：剩余显存 80-12-40=28GB，kv cache 默认占用 80%，即 28*0.8=22.4GB<br>   4、其他项<br>   是故总占用=Vit 权重占用 12GB+internlm 模型权重占用 40GB+kv cache 占用 22.4GB+其他项≥74.4GB<br>      对于使用 30%A100*1 (24GB 显存容量) 联合部署的显存情况 (23.8GB)：<br>   1、在 fp16 精度下，6BViT 模型权重占用 12GB：60×10^9 parameters×2 Bytes/parameter=12GB (ViT 使用精度为 fp16 的 pytorch 推理，量化只对 internlm 起效果)<br>   2、在 int4 精度下，19.86B≈20B 的 internlm 模型权重占用 10GB：200×10^9 parameters×0.5 Bytes/parameter=10GB<br>   3、kv cache 占用 0.2GB：剩余显存 24-12-10=2GB，kv cache 修改为占用 10%，即 2*0.1=0.2GB<br>   4、其他项 1.6GB<br>   是故 23.8GB=Vit 权重占用 12GB+internlm 模型权重占用 10GB+kv cache 占用 0.2GB+其他项 1.6GB </details>

如果此时推理图片，则会显示剩余显存不足，这是因为推理图片的时候 pytorch 会占用额外的激活显存，故有需要的小伙伴可以开启 50%A100 进行图片推理。

## <a id="3.2">3.2 LMDeploy API 部署 InternVL2</a>

具体封装操作与之前大同小异，仅仅在数个指令细节上作调整，故本章节大部分操作与 [2.1 LMDeploy API部署InternLM2.5](#2.1) 中几近完全一样，同学们可自行"依葫芦画瓢"，以下教程仅做参考。

通过以下命令启动 API 服务器，部署 InternVL2 模型：

```Python
lmdeploy serve api_server \
    /root/models/InternVL2-26B-w4a16-4bit/ \
    --model-format awq \
    --quant-policy 4 \
    --cache-max-entry-count 0.1 \
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
```

其余步骤与 [2.1.1 启动API服务器](#2.1.1)剩余内容一致。

命令行形式、Gradio 网页形式连接操作与

[2.1.2 以命令行形式连接API服务器](#2.1.2) 

[2.1.3 以Gradio网页形式连接API服务器](#2.1.3)

步骤流程、指令完全一致，不再赘述。

以下为 Gradio 网页形式连接成功后对话截图。

![img](https://raw.githubusercontent.com/BigWhiteFox/pictures/main/34.png)







```
(lmdeploy) root@intern-studio-50193904:~# lmdeploy chat /root/models/internlm2_5-7b-chat
chat_template_config:
ChatTemplateConfig(model_name='internlm2', system=None, meta_instruction=None, eosys=None, user=None, eoh=None, assistant=None, eoa=None, separator=None, capability='chat', stop_words=None)
engine_cfg:
TurbomindEngineConfig(model_name='/root/models/internlm2_5-7b-chat', model_format=None, tp=1, session_len=32768, max_batch_size=1, cache_max_entry_count=0.8, cache_block_seq_len=64, enable_prefix_caching=False, quant_policy=0, rope_scaling_factor=0.0, use_logn_attn=False, download_dir=None, revision=None, max_prefill_token_num=8192, num_tokens_per_iter=0, max_prefill_iters=1)
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/bin/lmdeploy", line 8, in <module>
    sys.exit(run())
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/entrypoint.py", line 36, in run
    args.run(args)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/cli.py", line 296, in chat
    run_chat(**kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/chat.py", line 113, in main
    tm_model = tm.TurboMind.from_pretrained(model_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 340, in from_pretrained
    return cls(model_path=pretrained_model_name_or_path,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 144, in __init__
    self.model_comm = self._from_hf(model_source=model_source,
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 251, in _from_hf
    self._create_weight(model_comm)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 170, in _create_weight
    future.result()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/root/.conda/envs/lmdeploy/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/root/.conda/envs/lmdeploy/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/turbomind/turbomind.py", line 163, in _create_weight_func
    model_comm.create_shared_weights(device_id, rank)
RuntimeError: [TM][ERROR] CUDA runtime error: out of memory /lmdeploy/src/turbomind/utils/memory_utils.cu:32 
```